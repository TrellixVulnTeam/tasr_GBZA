{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38c9ff6",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "TRAIN_FOLD = 0\n",
    "EXP_NAME = 'tasr_mt5_large_f0'\n",
    "MODEL_CHECKPOINT = 'google/mt5-large'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "GRAG_ACC_STEP = 1\n",
    "MAX_INPUT_LEN = 192\n",
    "MAX_TARGET_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import re\n",
    "import json\n",
    "import jiwer\n",
    "import logging\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModel, AutoTokenizer, MT5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bcf35c",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "with open('train_all.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e09a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "kf = KFold(n_splits=10, random_state=1998, shuffle=True)\n",
    "for i_fold, (train_index, valid_index) in enumerate(kf.split(data)):\n",
    "    \n",
    "    if i_fold != TRAIN_FOLD:\n",
    "        continue\n",
    "        \n",
    "    train_data = [data[idx] for idx in train_index]\n",
    "    valid_data = [data[idx] for idx in valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70983b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f6087",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# remove duplicate string and convert to huggingface dataset\n",
    "def list_drop_dup(data_list):\n",
    "    return list(dict.fromkeys(data_list))\n",
    "\n",
    "def to_hf_dataset(data, drop_duplicate=False):    \n",
    "    data_dict = {}\n",
    "    if drop_duplicate:\n",
    "        dd_sents = [list_drop_dup([s.replace(' ', '') for s in d['sentence_list']]) for d in data]\n",
    "        data_dict['asr_sentences'] = ['</s>'.join(ss) for ss in dd_sents]\n",
    "    else:\n",
    "        data_dict['asr_sentences'] = ['</s>'.join(d['sentence_list']).replace(' ', '') for d in data]\n",
    "    data_dict['ground_truth'] = [d['ground_truth_sentence'] for d in data]\n",
    "    \n",
    "    \n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7b537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = to_hf_dataset(train_data, drop_duplicate=True)\n",
    "valid_dataset = to_hf_dataset(valid_data, drop_duplicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efc3af",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f45f70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31845e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"asr_sentences\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LEN, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"ground_truth\"], max_length=MAX_TARGET_LEN, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"length\"] = [len(input_ids) for input_ids in model_inputs[\"input_ids\"]]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c55ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_train_datasets = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_valid_datasets = valid_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569bfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train_datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df468f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    max_length=MAX_TARGET_LEN,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4145b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set training hyperparameters\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    EXP_NAME,\n",
    "    \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,\n",
    "    save_steps=250,\n",
    "    \n",
    "    seed=87,\n",
    "    data_seed=87,\n",
    "    group_by_length=True,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='mcer',\n",
    "    greater_is_better=False,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "    gradient_accumulation_steps=GRAG_ACC_STEP,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    optim=\"adafactor\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.00,\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type='cosine',\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656131d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric functions\n",
    "cer = load_metric(\"cer\")\n",
    "\n",
    "def mcer(predictions, references):\n",
    "    return np.mean([jiwer.cer(ref, pred) for pred, ref in zip(predictions, references)])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "        print(list(zip(decoded_labels[:100], decoded_preds[:100])))\n",
    "        for idx, (gt, pred) in enumerate(zip(decoded_labels[:500], decoded_preds[:500])):\n",
    "            if gt != pred:\n",
    "                print(idx, gt, pred)\n",
    "    result = {}\n",
    "    result['cer'] = cer.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result['mcer'] = mcer(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c04a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset=tokenized_valid_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c178a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47002f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save_pretrained(f\"{EXP_NAME}/best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63494de",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608347c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "eval_result = trainer.evaluate(tokenized_valid_datasets)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c0580",
   "metadata": {},
   "source": [
    "# To ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model to onnx format for faster inference\n",
    "!cd onnxruntime/onnxruntime/python/tools/transformers/models/t5 && python convert_to_onnx.py \\\n",
    "-m $f\"{EXP_NAME}/best\" \\\n",
    "--output api/onnx_models/f0_best \\\n",
    "--use_gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
